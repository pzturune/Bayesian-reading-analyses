# Preliminaries

This is an example for an R Markdown document showing a basic
Bayesian regression.

First, we load the packages. If these are not installed, you can run
`install.packages(c("brms", "tidyverse"))` to install them.

```{r }
library(brms) # bayesian regression modelling with Stan
library(tidyverse) # package for manipulating data easily
```

# Data

In this example, we try to make a model that predicts ozone level from
different variables in the `airquality` dataset.

```{r }
data(airquality)

head(airquality)

```

# Model 1

## Choosing priors

Let's first consider the predictor `Wind` which is the windspeed in
mph and it's prediction of `Ozone` (parts per billion). We want to
come up with a reasonable value for how much a change in wind of 1mph
affects the ozone level. Without prior info, we can consider that it
is bidirectional, so we can center it at zero. If we want a weakly
informative prior something like normal(0, 50) might be good, as it
covers a large range of reasonable values.

```{r}
priors <- set_prior(
  "normal(0, 50)",
  class = "b",
  coef = "Wind"
)
```

## Specify the formula
```{r }
formula <- brms::bf(Ozone ~ Solar.R + Wind + Day)
#get_prior(Ozone ~ Solar + Wind + Day, data=airquality) #This is how I tried to find out more about the priors;
# didn't knit but executed separately
```

## Fit the model
```{r }
fit1 <- brms::brm(formula, prior = priors, data = airquality)
```

## View the model summary
```{r }
summary(fit1)
```

# Model 2

We can also specify the formula directly in the `brm()` function.

```{r}
fit2 <- brm(Ozone ~ Solar.R + Wind + Day + Temp + Month, data = airquality)

```

# Model comparison

We can compare the models by evaluating their performance using
leave-one-out cross-validation. This approximates how well new, unseen
data would be predicted by the model.

```{r}

fit1_loo <- loo(fit1)
print(fit1_loo)

fit2_loo <- loo(fit2)
print(fit2_loo)

loo_compare(list(fit1_loo, fit2_loo))

```

Here it seems that the second model is slightly better at predicting
unseen data (higher `elpd` value = expected log probability on new
data).
